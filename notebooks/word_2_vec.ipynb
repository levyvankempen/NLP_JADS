{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List\n",
    "\n",
    "# If you haven't already, you'll need to download these resources.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#import data\n",
    "prediction_df = pd.read_csv('../data/prediction_data_aapl.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preprocess dataframe such that the text of column \"content\" is a list of strings instead of normal letters.\n",
    "\n",
    "def preprocess_dataframe_content(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocesses each entry in the 'content' column of the given DataFrame by:\n",
    "    - Lowercasing\n",
    "    - Keeping only alphabetic characters\n",
    "    - Removing stopwords\n",
    "    - Lemmatizing\n",
    "    - Filtering out words with length less than 3\n",
    "    Tokenizes the preprocessed entry into a list of words.\n",
    "    Returns a list of lists where each inner list is a tokenized and preprocessed entry from the\n",
    "    'content' column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the 'content' column to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists, where each inner list is a tokenized and preprocessed entry from the\n",
    "    'content' column of the DataFrame.\n",
    "    \"\"\"\n",
    "    # Prepare lemmatizer and stopwords list\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Preprocess the content\n",
    "    processed_content = []\n",
    "    for content in df['content']:\n",
    "        # Keep only alphabetic characters and lowercased\n",
    "        tokens = re.sub('[^a-zA-Z\\s]', '', content.lower().strip()).split()\n",
    "        # Remove stopwords and short words, and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "        processed_content.append(tokens)\n",
    "\n",
    "    return processed_content\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#preprocess the prediction data\n",
    "prep_pred_data = preprocess_dataframe_content(prediction_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Given a context, predict a word --> kun je doen met context naar increase/decrease maar lijkt me ver gezocht\n",
    "cbow_model = Word2Vec(sentences = prep_pred_data,\n",
    "                      vector_size = 500,\n",
    "                      window = 5,\n",
    "                      min_count = 1,\n",
    "                      sg = 0) #sg=0 means Cbow, sg=1 means skipgram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Given a word, predict the context --> given apple, predict what the news will say\n",
    "skip_model = Word2Vec(sentences = prep_pred_data,\n",
    "                      vector_size = 500,\n",
    "                      window = 5,\n",
    "                      min_count = 1,\n",
    "                      sg = 1) #sg=0 means Cbow, sg=1 means skipgram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iphones', 0.6208415627479553), ('handset', 0.5964974761009216), ('smartphone', 0.5851163268089294), ('ipad', 0.5786470174789429), ('gadget', 0.5411442518234253), ('phone', 0.526934027671814), ('smartwatch', 0.5150953531265259), ('smartphones', 0.5022557377815247), ('galaxy', 0.49244457483291626), ('mac', 0.48796501755714417)]\n",
      "[('moregood', 0.3535393476486206), ('trampling', 0.30653002858161926), ('gainshares', 0.296678751707077), ('facebook', 0.2835407555103302), ('considerurban', 0.27017369866371155), ('considerworkday', 0.26064935326576233), ('verge', 0.2545115053653717), ('nflx', 0.2507176697254181), ('foramerican', 0.24650248885154724), ('explosion', 0.240297332406044)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow_model.wv.most_similar(\"iphone\", topn = 10))\n",
    "print(cbow_model.wv.most_similar(positive=[\"apple\"], negative = ['samsung'], topn = 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aapl', 0.6430505514144897), ('briskly', 0.6250367760658264), ('ascendance', 0.6167294979095459), ('appleapple', 0.6146665811538696), ('reignite', 0.5957915186882019), ('sliver', 0.5943598747253418), ('weathered', 0.5898398756980896), ('cannibalized', 0.5845192670822144), ('advanded', 0.5828044414520264), ('nkeheadquartered', 0.5826593637466431)]\n",
      "[('aapl', 0.29989495873451233), ('grub', 0.23178832232952118), ('biggie', 0.2272990494966507), ('insane', 0.2268953174352646), ('ignoring', 0.22467628121376038), ('ihrt', 0.2210230827331543), ('beast', 0.2204151749610901), ('bleed', 0.21781058609485626), ('judging', 0.21624860167503357), ('absurdly', 0.21575164794921875)]\n"
     ]
    }
   ],
   "source": [
    "print(skip_model.wv.most_similar(\"apple\", topn = 10))\n",
    "print(skip_model.wv.most_similar(positive=[\"apple\"], negative = ['samsung'], topn = 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.3813889e+00, -8.2837516e-01, -4.9135578e-01, ...,\n         9.5497245e-01,  1.3172935e+00,  9.5718837e-01],\n       [-8.4077632e-01,  1.3270417e-01, -7.1590400e-01, ...,\n         1.9882495e-02,  6.4561683e-01,  3.5624686e-01],\n       [-3.1210819e-01, -1.3658292e+00,  2.1714839e-01, ...,\n         7.0638359e-01,  1.5217949e-01,  7.7731621e-01],\n       ...,\n       [ 3.4957996e-03,  1.0397855e-02,  2.2640338e-03, ...,\n        -1.4712235e-02, -1.5349389e-02,  5.7460245e-04],\n       [ 5.9544896e-03,  4.7844159e-03,  3.3711873e-03, ...,\n         5.7485951e-03, -7.9359729e-03,  9.6770925e-03],\n       [ 9.7475816e-03, -1.0345847e-02, -6.2139700e-03, ...,\n         4.5812638e-03, -8.1861659e-04,  5.8980617e-03]], dtype=float32)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_cbow = cbow_model.wv.vectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}