{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20182877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# If you haven't already, you'll need to download\n",
    "# these resources.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "skip_model = Word2Vec.load(\"../models/cbow_model_2min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv('../data/prediction_data_aapl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#preprocess dataframe such that the text of column \"content\" is a list of strings instead of normal letters.\n",
    "\n",
    "def preprocess_dataframe_content(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocesses each entry in the 'content' column of the given DataFrame by:\n",
    "    - Lowercasing\n",
    "    - Keeping only alphabetic characters\n",
    "    - Removing stopwords\n",
    "    - Lemmatizing\n",
    "    - Filtering out words with length less than 3\n",
    "    Tokenizes the preprocessed entry into a list of words.\n",
    "    Returns a list of lists where each inner list is a tokenized and preprocessed entry from the\n",
    "    'content' column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the 'content' column to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists, where each inner list is a tokenized and preprocessed entry from the\n",
    "    'content' column of the DataFrame.\n",
    "    \"\"\"\n",
    "    # Prepare lemmatizer and stopwords list\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Preprocess the content\n",
    "    processed_content = []\n",
    "    for content in df['content']:\n",
    "        # Keep only alphabetic characters and lowercased\n",
    "        tokens = re.sub('[^a-zA-Z\\s]', '', content.lower().strip()).split()\n",
    "        # Remove stopwords and short words, and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "        processed_content.append(tokens)\n",
    "\n",
    "    return processed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#preprocess the prediction data\n",
    "prep_pred_data = preprocess_dataframe_content(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#split training and testing data\n",
    "train_data, test_data = train_test_split(prep_pred_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "document_vectors = []\n",
    "\n",
    "# Iterate through each document in your dataset\n",
    "for document in prep_pred_data:\n",
    "    # Initialize an empty vector for the document\n",
    "    doc_vector = np.zeros(skip_model.vector_size)\n",
    "    num_words = 0\n",
    "    for word in document:\n",
    "        if word in skip_model.wv:\n",
    "            doc_vector += skip_model.wv[word]\n",
    "            num_words += 1\n",
    "    if num_words > 0:\n",
    "        doc_vector /= num_words  # Take the average of word vectors in the document\n",
    "    document_vectors.append(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "document_vectors_dataframe = pd.DataFrame(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\n0      0.168224 -0.495679  0.324030 -0.242480  0.184591  0.374841 -0.036248   \n1     -0.012605 -0.394349  0.097361 -0.225669 -0.199009  0.392812 -0.153641   \n2     -0.047816 -0.060572  0.057269 -0.000904 -0.292328  0.483402 -0.015738   \n3     -0.040624 -0.168761  0.058535  0.057410 -0.100838  0.292741 -0.004191   \n4      0.089348 -0.326622  0.131642 -0.150949 -0.131480  0.549980  0.038805   \n...         ...       ...       ...       ...       ...       ...       ...   \n18067 -0.095696 -0.171368  0.149107 -0.164140 -0.111491  0.243613 -0.036502   \n18068 -0.002159 -0.024052 -0.030515  0.008726 -0.373765  0.427209 -0.054720   \n18069  0.001782 -0.150355  0.250220 -0.084058 -0.265737  0.535945  0.014501   \n18070  0.287233 -0.275369  0.172296  0.050259 -0.349938  0.312524  0.019061   \n18071 -0.228906 -0.087511  0.108614 -0.245653 -0.008118  0.293103  0.086047   \n\n            7         8         9    ...       290       291       292  \\\n0     -0.332221 -0.599187  0.368172  ...  0.116010 -0.646997 -0.111996   \n1      0.342992 -0.346285  0.136275  ...  0.252347 -0.564153 -0.362388   \n2      0.226332 -0.287929  0.094340  ...  0.144032 -0.367153 -0.108658   \n3      0.043423 -0.328297  0.279242  ...  0.188750 -0.409938 -0.135549   \n4     -0.056795 -0.331820  0.276895  ...  0.125006 -0.406472 -0.159782   \n...         ...       ...       ...  ...       ...       ...       ...   \n18067  0.238864 -0.068185  0.404670  ...  0.256503 -0.290045 -0.146078   \n18068  0.070499 -0.352737  0.225969  ...  0.260338 -0.230413 -0.302271   \n18069 -0.003547 -0.236052  0.155300  ...  0.090240 -0.288605 -0.130172   \n18070 -0.355033 -0.593154  0.604696  ...  0.115085 -0.467677 -0.249983   \n18071  0.246072 -0.362599  0.569807  ...  0.486536 -0.177991 -0.201380   \n\n            293       294       295       296       297       298       299  \n0     -0.299829 -0.443257 -0.242020  0.841042 -0.073041 -0.109485 -0.098358  \n1     -0.179683 -0.026317  0.270566  0.617830  0.184016 -0.020996 -0.073160  \n2     -0.241499 -0.246777  0.256733  0.619212 -0.057776  0.384899 -0.119208  \n3     -0.145407 -0.212148  0.148339  0.432560 -0.214075  0.019210 -0.120047  \n4     -0.189099 -0.325308  0.047118  0.610220 -0.052178  0.079585 -0.173855  \n...         ...       ...       ...       ...       ...       ...       ...  \n18067 -0.302595 -0.104610  0.118958  0.441293  0.070423  0.260583  0.016124  \n18068 -0.414318 -0.309454  0.150445  0.448909  0.129060  0.384882 -0.178881  \n18069 -0.184711 -0.374830  0.127884  0.660305 -0.004154  0.464820 -0.106518  \n18070 -0.779245 -0.383697  0.020691  0.649797 -0.141340  0.126768 -0.051663  \n18071 -0.261182 -0.163506  0.237526  0.460088  0.116863  0.234689 -0.134007  \n\n[18072 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.168224</td>\n      <td>-0.495679</td>\n      <td>0.324030</td>\n      <td>-0.242480</td>\n      <td>0.184591</td>\n      <td>0.374841</td>\n      <td>-0.036248</td>\n      <td>-0.332221</td>\n      <td>-0.599187</td>\n      <td>0.368172</td>\n      <td>...</td>\n      <td>0.116010</td>\n      <td>-0.646997</td>\n      <td>-0.111996</td>\n      <td>-0.299829</td>\n      <td>-0.443257</td>\n      <td>-0.242020</td>\n      <td>0.841042</td>\n      <td>-0.073041</td>\n      <td>-0.109485</td>\n      <td>-0.098358</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.012605</td>\n      <td>-0.394349</td>\n      <td>0.097361</td>\n      <td>-0.225669</td>\n      <td>-0.199009</td>\n      <td>0.392812</td>\n      <td>-0.153641</td>\n      <td>0.342992</td>\n      <td>-0.346285</td>\n      <td>0.136275</td>\n      <td>...</td>\n      <td>0.252347</td>\n      <td>-0.564153</td>\n      <td>-0.362388</td>\n      <td>-0.179683</td>\n      <td>-0.026317</td>\n      <td>0.270566</td>\n      <td>0.617830</td>\n      <td>0.184016</td>\n      <td>-0.020996</td>\n      <td>-0.073160</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.047816</td>\n      <td>-0.060572</td>\n      <td>0.057269</td>\n      <td>-0.000904</td>\n      <td>-0.292328</td>\n      <td>0.483402</td>\n      <td>-0.015738</td>\n      <td>0.226332</td>\n      <td>-0.287929</td>\n      <td>0.094340</td>\n      <td>...</td>\n      <td>0.144032</td>\n      <td>-0.367153</td>\n      <td>-0.108658</td>\n      <td>-0.241499</td>\n      <td>-0.246777</td>\n      <td>0.256733</td>\n      <td>0.619212</td>\n      <td>-0.057776</td>\n      <td>0.384899</td>\n      <td>-0.119208</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.040624</td>\n      <td>-0.168761</td>\n      <td>0.058535</td>\n      <td>0.057410</td>\n      <td>-0.100838</td>\n      <td>0.292741</td>\n      <td>-0.004191</td>\n      <td>0.043423</td>\n      <td>-0.328297</td>\n      <td>0.279242</td>\n      <td>...</td>\n      <td>0.188750</td>\n      <td>-0.409938</td>\n      <td>-0.135549</td>\n      <td>-0.145407</td>\n      <td>-0.212148</td>\n      <td>0.148339</td>\n      <td>0.432560</td>\n      <td>-0.214075</td>\n      <td>0.019210</td>\n      <td>-0.120047</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.089348</td>\n      <td>-0.326622</td>\n      <td>0.131642</td>\n      <td>-0.150949</td>\n      <td>-0.131480</td>\n      <td>0.549980</td>\n      <td>0.038805</td>\n      <td>-0.056795</td>\n      <td>-0.331820</td>\n      <td>0.276895</td>\n      <td>...</td>\n      <td>0.125006</td>\n      <td>-0.406472</td>\n      <td>-0.159782</td>\n      <td>-0.189099</td>\n      <td>-0.325308</td>\n      <td>0.047118</td>\n      <td>0.610220</td>\n      <td>-0.052178</td>\n      <td>0.079585</td>\n      <td>-0.173855</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18067</th>\n      <td>-0.095696</td>\n      <td>-0.171368</td>\n      <td>0.149107</td>\n      <td>-0.164140</td>\n      <td>-0.111491</td>\n      <td>0.243613</td>\n      <td>-0.036502</td>\n      <td>0.238864</td>\n      <td>-0.068185</td>\n      <td>0.404670</td>\n      <td>...</td>\n      <td>0.256503</td>\n      <td>-0.290045</td>\n      <td>-0.146078</td>\n      <td>-0.302595</td>\n      <td>-0.104610</td>\n      <td>0.118958</td>\n      <td>0.441293</td>\n      <td>0.070423</td>\n      <td>0.260583</td>\n      <td>0.016124</td>\n    </tr>\n    <tr>\n      <th>18068</th>\n      <td>-0.002159</td>\n      <td>-0.024052</td>\n      <td>-0.030515</td>\n      <td>0.008726</td>\n      <td>-0.373765</td>\n      <td>0.427209</td>\n      <td>-0.054720</td>\n      <td>0.070499</td>\n      <td>-0.352737</td>\n      <td>0.225969</td>\n      <td>...</td>\n      <td>0.260338</td>\n      <td>-0.230413</td>\n      <td>-0.302271</td>\n      <td>-0.414318</td>\n      <td>-0.309454</td>\n      <td>0.150445</td>\n      <td>0.448909</td>\n      <td>0.129060</td>\n      <td>0.384882</td>\n      <td>-0.178881</td>\n    </tr>\n    <tr>\n      <th>18069</th>\n      <td>0.001782</td>\n      <td>-0.150355</td>\n      <td>0.250220</td>\n      <td>-0.084058</td>\n      <td>-0.265737</td>\n      <td>0.535945</td>\n      <td>0.014501</td>\n      <td>-0.003547</td>\n      <td>-0.236052</td>\n      <td>0.155300</td>\n      <td>...</td>\n      <td>0.090240</td>\n      <td>-0.288605</td>\n      <td>-0.130172</td>\n      <td>-0.184711</td>\n      <td>-0.374830</td>\n      <td>0.127884</td>\n      <td>0.660305</td>\n      <td>-0.004154</td>\n      <td>0.464820</td>\n      <td>-0.106518</td>\n    </tr>\n    <tr>\n      <th>18070</th>\n      <td>0.287233</td>\n      <td>-0.275369</td>\n      <td>0.172296</td>\n      <td>0.050259</td>\n      <td>-0.349938</td>\n      <td>0.312524</td>\n      <td>0.019061</td>\n      <td>-0.355033</td>\n      <td>-0.593154</td>\n      <td>0.604696</td>\n      <td>...</td>\n      <td>0.115085</td>\n      <td>-0.467677</td>\n      <td>-0.249983</td>\n      <td>-0.779245</td>\n      <td>-0.383697</td>\n      <td>0.020691</td>\n      <td>0.649797</td>\n      <td>-0.141340</td>\n      <td>0.126768</td>\n      <td>-0.051663</td>\n    </tr>\n    <tr>\n      <th>18071</th>\n      <td>-0.228906</td>\n      <td>-0.087511</td>\n      <td>0.108614</td>\n      <td>-0.245653</td>\n      <td>-0.008118</td>\n      <td>0.293103</td>\n      <td>0.086047</td>\n      <td>0.246072</td>\n      <td>-0.362599</td>\n      <td>0.569807</td>\n      <td>...</td>\n      <td>0.486536</td>\n      <td>-0.177991</td>\n      <td>-0.201380</td>\n      <td>-0.261182</td>\n      <td>-0.163506</td>\n      <td>0.237526</td>\n      <td>0.460088</td>\n      <td>0.116863</td>\n      <td>0.234689</td>\n      <td>-0.134007</td>\n    </tr>\n  </tbody>\n</table>\n<p>18072 rows × 300 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0      id ticker  \\\n0           49181  270698   AAPL   \n1           49182  270699   AAPL   \n2           49183  270700   AAPL   \n3           49184  270701   AAPL   \n4           49185  270702   AAPL   \n...           ...     ...    ...   \n18067       69407  290924   AAPL   \n18068       69408  290925   AAPL   \n18069       69409  290926   AAPL   \n18070       69410  290927   AAPL   \n18071       69411  290928   AAPL   \n\n                                                   title category  \\\n0              JPMorgan cautious ahead of Apple earnings     news   \n1            FAANG s Fall  but Get Some Wall Street Love     news   \n2      Wall Street tumbles as virus fuels economic worry     news   \n3      Earnings Watch  Apple and AMD to take earnings...     news   \n4            Day Ahead  Top 3 Things to Watch for Jan 28     news   \n...                                                  ...      ...   \n18067               Waiting For Direction On The Markets  opinion   \n18068  Mid Year Update  U S  And Canadian Stock Marke...  opinion   \n18069               Summer Heat Scorches Europe And U S   opinion   \n18070     Apple Earnings Preview  Quarterly Dip On Deck   opinion   \n18071                         Trade Apple After Earnings  opinion   \n\n                                                 content release_date  \\\n0      jpmorgan lift apple aapl target ahead tomorrow...   2020-01-28   \n1      kim khan investing com faang stock predictably...   2020-01-28   \n2      chuck mikolajczak new york reuters stock suffe...   2020-01-28   \n3      two best performing tech stock set report resu...   2020-01-28   \n4      yasin ebrahim kim khan apple ready earnings in...   2020-01-28   \n...                                                  ...          ...   \n18067  stock market difficult one trader investor ali...   2012-07-16   \n18068  tsx index leading canadian stock outperformed ...   2012-07-19   \n18069  europe flare summer heat continues summer heat...   2012-07-23   \n18070  last quarter apple aapl reported best quarter ...   2012-07-23   \n18071  may look like spider web mishmash trendlines m...   2012-07-23   \n\n                    provider  \\\n0              Seeking Alpha   \n1              Investing.com   \n2                    Reuters   \n3                MarketWatch   \n4              Investing.com   \n...                      ...   \n18067                Cam Hui   \n18068  Baskin Financial Blog   \n18069           John Nyaradi   \n18070             David Dyer   \n18071      Abigail Doolittle   \n\n                                                     url  article_id  \\\n0                                 https://invst.ly/pnjv8     2068762   \n1      https://www.investing.com/news/stock-market-ne...     2068765   \n2      https://www.investing.com/news/stock-market-ne...     2068311   \n3                                 https://invst.ly/pnlbs     2068906   \n4      https://www.investing.com/news/stock-market-ne...     2068907   \n...                                                  ...         ...   \n18067  https://www.investing.com/analysis/waiting-for...      129680   \n18068  https://www.investing.com/analysis/mid-year-up...      130056   \n18069  https://www.investing.com/analysis/summer-heat...      130439   \n18070  https://www.investing.com/analysis/apple-earni...      130458   \n18071  https://www.investing.com/analysis/trade-apple...      130440   \n\n             Date  stock_increase  \n0      2020-01-28             1.0  \n1      2020-01-28             1.0  \n2      2020-01-28             1.0  \n3      2020-01-28             1.0  \n4      2020-01-28             1.0  \n...           ...             ...  \n18067  2012-07-16             1.0  \n18068  2012-07-19             1.0  \n18069  2012-07-23             1.0  \n18070  2012-07-23             1.0  \n18071  2012-07-23             1.0  \n\n[18072 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>ticker</th>\n      <th>title</th>\n      <th>category</th>\n      <th>content</th>\n      <th>release_date</th>\n      <th>provider</th>\n      <th>url</th>\n      <th>article_id</th>\n      <th>Date</th>\n      <th>stock_increase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>49181</td>\n      <td>270698</td>\n      <td>AAPL</td>\n      <td>JPMorgan cautious ahead of Apple earnings</td>\n      <td>news</td>\n      <td>jpmorgan lift apple aapl target ahead tomorrow...</td>\n      <td>2020-01-28</td>\n      <td>Seeking Alpha</td>\n      <td>https://invst.ly/pnjv8</td>\n      <td>2068762</td>\n      <td>2020-01-28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49182</td>\n      <td>270699</td>\n      <td>AAPL</td>\n      <td>FAANG s Fall  but Get Some Wall Street Love</td>\n      <td>news</td>\n      <td>kim khan investing com faang stock predictably...</td>\n      <td>2020-01-28</td>\n      <td>Investing.com</td>\n      <td>https://www.investing.com/news/stock-market-ne...</td>\n      <td>2068765</td>\n      <td>2020-01-28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49183</td>\n      <td>270700</td>\n      <td>AAPL</td>\n      <td>Wall Street tumbles as virus fuels economic worry</td>\n      <td>news</td>\n      <td>chuck mikolajczak new york reuters stock suffe...</td>\n      <td>2020-01-28</td>\n      <td>Reuters</td>\n      <td>https://www.investing.com/news/stock-market-ne...</td>\n      <td>2068311</td>\n      <td>2020-01-28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>49184</td>\n      <td>270701</td>\n      <td>AAPL</td>\n      <td>Earnings Watch  Apple and AMD to take earnings...</td>\n      <td>news</td>\n      <td>two best performing tech stock set report resu...</td>\n      <td>2020-01-28</td>\n      <td>MarketWatch</td>\n      <td>https://invst.ly/pnlbs</td>\n      <td>2068906</td>\n      <td>2020-01-28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49185</td>\n      <td>270702</td>\n      <td>AAPL</td>\n      <td>Day Ahead  Top 3 Things to Watch for Jan 28</td>\n      <td>news</td>\n      <td>yasin ebrahim kim khan apple ready earnings in...</td>\n      <td>2020-01-28</td>\n      <td>Investing.com</td>\n      <td>https://www.investing.com/news/stock-market-ne...</td>\n      <td>2068907</td>\n      <td>2020-01-28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18067</th>\n      <td>69407</td>\n      <td>290924</td>\n      <td>AAPL</td>\n      <td>Waiting For Direction On The Markets</td>\n      <td>opinion</td>\n      <td>stock market difficult one trader investor ali...</td>\n      <td>2012-07-16</td>\n      <td>Cam Hui</td>\n      <td>https://www.investing.com/analysis/waiting-for...</td>\n      <td>129680</td>\n      <td>2012-07-16</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18068</th>\n      <td>69408</td>\n      <td>290925</td>\n      <td>AAPL</td>\n      <td>Mid Year Update  U S  And Canadian Stock Marke...</td>\n      <td>opinion</td>\n      <td>tsx index leading canadian stock outperformed ...</td>\n      <td>2012-07-19</td>\n      <td>Baskin Financial Blog</td>\n      <td>https://www.investing.com/analysis/mid-year-up...</td>\n      <td>130056</td>\n      <td>2012-07-19</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18069</th>\n      <td>69409</td>\n      <td>290926</td>\n      <td>AAPL</td>\n      <td>Summer Heat Scorches Europe And U S</td>\n      <td>opinion</td>\n      <td>europe flare summer heat continues summer heat...</td>\n      <td>2012-07-23</td>\n      <td>John Nyaradi</td>\n      <td>https://www.investing.com/analysis/summer-heat...</td>\n      <td>130439</td>\n      <td>2012-07-23</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18070</th>\n      <td>69410</td>\n      <td>290927</td>\n      <td>AAPL</td>\n      <td>Apple Earnings Preview  Quarterly Dip On Deck</td>\n      <td>opinion</td>\n      <td>last quarter apple aapl reported best quarter ...</td>\n      <td>2012-07-23</td>\n      <td>David Dyer</td>\n      <td>https://www.investing.com/analysis/apple-earni...</td>\n      <td>130458</td>\n      <td>2012-07-23</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18071</th>\n      <td>69411</td>\n      <td>290928</td>\n      <td>AAPL</td>\n      <td>Trade Apple After Earnings</td>\n      <td>opinion</td>\n      <td>may look like spider web mishmash trendlines m...</td>\n      <td>2012-07-23</td>\n      <td>Abigail Doolittle</td>\n      <td>https://www.investing.com/analysis/trade-apple...</td>\n      <td>130440</td>\n      <td>2012-07-23</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>18072 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the 'stock_increase' column from prediction_df\n",
    "prediction_df = prediction_df[['stock_increase']]\n",
    "\n",
    "# Merge the two DataFrames side by side\n",
    "merged_df = pd.concat([prediction_df, document_vectors_dataframe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       stock_increase         0         1         2         3         4  \\\n0                 1.0  0.168224 -0.495679  0.324030 -0.242480  0.184591   \n1                 1.0 -0.012605 -0.394349  0.097361 -0.225669 -0.199009   \n2                 1.0 -0.047816 -0.060572  0.057269 -0.000904 -0.292328   \n3                 1.0 -0.040624 -0.168761  0.058535  0.057410 -0.100838   \n4                 1.0  0.089348 -0.326622  0.131642 -0.150949 -0.131480   \n...               ...       ...       ...       ...       ...       ...   \n18067             1.0 -0.095696 -0.171368  0.149107 -0.164140 -0.111491   \n18068             1.0 -0.002159 -0.024052 -0.030515  0.008726 -0.373765   \n18069             1.0  0.001782 -0.150355  0.250220 -0.084058 -0.265737   \n18070             1.0  0.287233 -0.275369  0.172296  0.050259 -0.349938   \n18071             1.0 -0.228906 -0.087511  0.108614 -0.245653 -0.008118   \n\n              5         6         7         8  ...       290       291  \\\n0      0.374841 -0.036248 -0.332221 -0.599187  ...  0.116010 -0.646997   \n1      0.392812 -0.153641  0.342992 -0.346285  ...  0.252347 -0.564153   \n2      0.483402 -0.015738  0.226332 -0.287929  ...  0.144032 -0.367153   \n3      0.292741 -0.004191  0.043423 -0.328297  ...  0.188750 -0.409938   \n4      0.549980  0.038805 -0.056795 -0.331820  ...  0.125006 -0.406472   \n...         ...       ...       ...       ...  ...       ...       ...   \n18067  0.243613 -0.036502  0.238864 -0.068185  ...  0.256503 -0.290045   \n18068  0.427209 -0.054720  0.070499 -0.352737  ...  0.260338 -0.230413   \n18069  0.535945  0.014501 -0.003547 -0.236052  ...  0.090240 -0.288605   \n18070  0.312524  0.019061 -0.355033 -0.593154  ...  0.115085 -0.467677   \n18071  0.293103  0.086047  0.246072 -0.362599  ...  0.486536 -0.177991   \n\n            292       293       294       295       296       297       298  \\\n0     -0.111996 -0.299829 -0.443257 -0.242020  0.841042 -0.073041 -0.109485   \n1     -0.362388 -0.179683 -0.026317  0.270566  0.617830  0.184016 -0.020996   \n2     -0.108658 -0.241499 -0.246777  0.256733  0.619212 -0.057776  0.384899   \n3     -0.135549 -0.145407 -0.212148  0.148339  0.432560 -0.214075  0.019210   \n4     -0.159782 -0.189099 -0.325308  0.047118  0.610220 -0.052178  0.079585   \n...         ...       ...       ...       ...       ...       ...       ...   \n18067 -0.146078 -0.302595 -0.104610  0.118958  0.441293  0.070423  0.260583   \n18068 -0.302271 -0.414318 -0.309454  0.150445  0.448909  0.129060  0.384882   \n18069 -0.130172 -0.184711 -0.374830  0.127884  0.660305 -0.004154  0.464820   \n18070 -0.249983 -0.779245 -0.383697  0.020691  0.649797 -0.141340  0.126768   \n18071 -0.201380 -0.261182 -0.163506  0.237526  0.460088  0.116863  0.234689   \n\n            299  \n0     -0.098358  \n1     -0.073160  \n2     -0.119208  \n3     -0.120047  \n4     -0.173855  \n...         ...  \n18067  0.016124  \n18068 -0.178881  \n18069 -0.106518  \n18070 -0.051663  \n18071 -0.134007  \n\n[18072 rows x 301 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_increase</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.168224</td>\n      <td>-0.495679</td>\n      <td>0.324030</td>\n      <td>-0.242480</td>\n      <td>0.184591</td>\n      <td>0.374841</td>\n      <td>-0.036248</td>\n      <td>-0.332221</td>\n      <td>-0.599187</td>\n      <td>...</td>\n      <td>0.116010</td>\n      <td>-0.646997</td>\n      <td>-0.111996</td>\n      <td>-0.299829</td>\n      <td>-0.443257</td>\n      <td>-0.242020</td>\n      <td>0.841042</td>\n      <td>-0.073041</td>\n      <td>-0.109485</td>\n      <td>-0.098358</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>-0.012605</td>\n      <td>-0.394349</td>\n      <td>0.097361</td>\n      <td>-0.225669</td>\n      <td>-0.199009</td>\n      <td>0.392812</td>\n      <td>-0.153641</td>\n      <td>0.342992</td>\n      <td>-0.346285</td>\n      <td>...</td>\n      <td>0.252347</td>\n      <td>-0.564153</td>\n      <td>-0.362388</td>\n      <td>-0.179683</td>\n      <td>-0.026317</td>\n      <td>0.270566</td>\n      <td>0.617830</td>\n      <td>0.184016</td>\n      <td>-0.020996</td>\n      <td>-0.073160</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-0.047816</td>\n      <td>-0.060572</td>\n      <td>0.057269</td>\n      <td>-0.000904</td>\n      <td>-0.292328</td>\n      <td>0.483402</td>\n      <td>-0.015738</td>\n      <td>0.226332</td>\n      <td>-0.287929</td>\n      <td>...</td>\n      <td>0.144032</td>\n      <td>-0.367153</td>\n      <td>-0.108658</td>\n      <td>-0.241499</td>\n      <td>-0.246777</td>\n      <td>0.256733</td>\n      <td>0.619212</td>\n      <td>-0.057776</td>\n      <td>0.384899</td>\n      <td>-0.119208</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.040624</td>\n      <td>-0.168761</td>\n      <td>0.058535</td>\n      <td>0.057410</td>\n      <td>-0.100838</td>\n      <td>0.292741</td>\n      <td>-0.004191</td>\n      <td>0.043423</td>\n      <td>-0.328297</td>\n      <td>...</td>\n      <td>0.188750</td>\n      <td>-0.409938</td>\n      <td>-0.135549</td>\n      <td>-0.145407</td>\n      <td>-0.212148</td>\n      <td>0.148339</td>\n      <td>0.432560</td>\n      <td>-0.214075</td>\n      <td>0.019210</td>\n      <td>-0.120047</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.089348</td>\n      <td>-0.326622</td>\n      <td>0.131642</td>\n      <td>-0.150949</td>\n      <td>-0.131480</td>\n      <td>0.549980</td>\n      <td>0.038805</td>\n      <td>-0.056795</td>\n      <td>-0.331820</td>\n      <td>...</td>\n      <td>0.125006</td>\n      <td>-0.406472</td>\n      <td>-0.159782</td>\n      <td>-0.189099</td>\n      <td>-0.325308</td>\n      <td>0.047118</td>\n      <td>0.610220</td>\n      <td>-0.052178</td>\n      <td>0.079585</td>\n      <td>-0.173855</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18067</th>\n      <td>1.0</td>\n      <td>-0.095696</td>\n      <td>-0.171368</td>\n      <td>0.149107</td>\n      <td>-0.164140</td>\n      <td>-0.111491</td>\n      <td>0.243613</td>\n      <td>-0.036502</td>\n      <td>0.238864</td>\n      <td>-0.068185</td>\n      <td>...</td>\n      <td>0.256503</td>\n      <td>-0.290045</td>\n      <td>-0.146078</td>\n      <td>-0.302595</td>\n      <td>-0.104610</td>\n      <td>0.118958</td>\n      <td>0.441293</td>\n      <td>0.070423</td>\n      <td>0.260583</td>\n      <td>0.016124</td>\n    </tr>\n    <tr>\n      <th>18068</th>\n      <td>1.0</td>\n      <td>-0.002159</td>\n      <td>-0.024052</td>\n      <td>-0.030515</td>\n      <td>0.008726</td>\n      <td>-0.373765</td>\n      <td>0.427209</td>\n      <td>-0.054720</td>\n      <td>0.070499</td>\n      <td>-0.352737</td>\n      <td>...</td>\n      <td>0.260338</td>\n      <td>-0.230413</td>\n      <td>-0.302271</td>\n      <td>-0.414318</td>\n      <td>-0.309454</td>\n      <td>0.150445</td>\n      <td>0.448909</td>\n      <td>0.129060</td>\n      <td>0.384882</td>\n      <td>-0.178881</td>\n    </tr>\n    <tr>\n      <th>18069</th>\n      <td>1.0</td>\n      <td>0.001782</td>\n      <td>-0.150355</td>\n      <td>0.250220</td>\n      <td>-0.084058</td>\n      <td>-0.265737</td>\n      <td>0.535945</td>\n      <td>0.014501</td>\n      <td>-0.003547</td>\n      <td>-0.236052</td>\n      <td>...</td>\n      <td>0.090240</td>\n      <td>-0.288605</td>\n      <td>-0.130172</td>\n      <td>-0.184711</td>\n      <td>-0.374830</td>\n      <td>0.127884</td>\n      <td>0.660305</td>\n      <td>-0.004154</td>\n      <td>0.464820</td>\n      <td>-0.106518</td>\n    </tr>\n    <tr>\n      <th>18070</th>\n      <td>1.0</td>\n      <td>0.287233</td>\n      <td>-0.275369</td>\n      <td>0.172296</td>\n      <td>0.050259</td>\n      <td>-0.349938</td>\n      <td>0.312524</td>\n      <td>0.019061</td>\n      <td>-0.355033</td>\n      <td>-0.593154</td>\n      <td>...</td>\n      <td>0.115085</td>\n      <td>-0.467677</td>\n      <td>-0.249983</td>\n      <td>-0.779245</td>\n      <td>-0.383697</td>\n      <td>0.020691</td>\n      <td>0.649797</td>\n      <td>-0.141340</td>\n      <td>0.126768</td>\n      <td>-0.051663</td>\n    </tr>\n    <tr>\n      <th>18071</th>\n      <td>1.0</td>\n      <td>-0.228906</td>\n      <td>-0.087511</td>\n      <td>0.108614</td>\n      <td>-0.245653</td>\n      <td>-0.008118</td>\n      <td>0.293103</td>\n      <td>0.086047</td>\n      <td>0.246072</td>\n      <td>-0.362599</td>\n      <td>...</td>\n      <td>0.486536</td>\n      <td>-0.177991</td>\n      <td>-0.201380</td>\n      <td>-0.261182</td>\n      <td>-0.163506</td>\n      <td>0.237526</td>\n      <td>0.460088</td>\n      <td>0.116863</td>\n      <td>0.234689</td>\n      <td>-0.134007</td>\n    </tr>\n  </tbody>\n</table>\n<p>18072 rows × 301 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features are all columns except 'stock_increase', and target is 'stock_increase'\n",
    "X = merged_df.drop(columns=['stock_increase'])\n",
    "y = merged_df['stock_increase']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "GaussianNB()",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.57%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.62      0.54      1705\n",
      "         1.0       0.54      0.40      0.46      1910\n",
      "\n",
      "    accuracy                           0.51      3615\n",
      "   macro avg       0.51      0.51      0.50      3615\n",
      "weighted avg       0.51      0.51      0.50      3615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.0    9645\n0.0    8427\nName: stock_increase, dtype: int64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"stock_increase\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.0    7735\n0.0    6722\nName: stock_increase, dtype: int64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.0    1910\n0.0    1705\nName: stock_increase, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: [0.5038036  0.52593361 0.50605327 0.50328606 0.50501557]\n",
      "Mean CV Accuracy: 0.5088184201171854 +/- 0.008611538239202049\n",
      "0.50567081604426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.62      0.54      1705\n",
      "         1.0       0.54      0.40      0.46      1910\n",
      "\n",
      "    accuracy                           0.51      3615\n",
      "   macro avg       0.51      0.51      0.50      3615\n",
      "weighted avg       0.51      0.51      0.50      3615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "scores = cross_val_score(gnb, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validated scores: {scores}\")\n",
    "print(f\"Mean CV Accuracy: {scores.mean()} +/- {scores.std()}\")\n",
    "\n",
    "# Train on the full training set and test\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: [0.54426003 0.52524205 0.52646143 0.53164995 0.53649256]\n",
      "Mean CV Accuracy: 0.5328212035922041 +/- 0.006979795482774591\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1., 0., 0., ..., 0., 0., 1.])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X = merged_df.drop('stock_increase', axis=1)\n",
    "y = merged_df['stock_increase']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest with Cross-Validation\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validated scores: {scores}\")\n",
    "print(f\"Mean CV Accuracy: {scores.mean()} +/- {scores.std()}\")\n",
    "\n",
    "# Train on the full training set and test\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50567081604426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.62      0.54      1705\n",
      "         1.0       0.54      0.40      0.46      1910\n",
      "\n",
      "    accuracy                           0.51      3615\n",
      "   macro avg       0.51      0.51      0.50      3615\n",
      "weighted avg       0.51      0.51      0.50      3615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'rf__n_estimators': randint(10, 200),   # Number of trees in the forest\n",
    "    'rf__max_features': ['auto', 'sqrt', 'log2'],  # Number of features for best split\n",
    "    'rf__max_depth': randint(1, 20),   # Depth of the tree\n",
    "    'rf__bootstrap': [True, False],  # Method of selecting samples for training each tree\n",
    "    'rf__class_weight': ['balanced', None]   # Weights associated with classes\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "10 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.52839501 0.51276229 0.53538082 0.52839444 0.53503495 0.53046932\n",
      " 0.53157718 0.52811779 0.50826534 0.51836469 0.52804839 0.50259364\n",
      " 0.53053871 0.50785002 0.53233749 0.53406745 0.53600325 0.51933305\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[('rf', RandomForestClassifier())]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'rf__bootstrap': [True, False],\n                                        'rf__class_weight': ['balanced', None],\n                                        'rf__max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C1F0>,\n                                        'rf__max_features': ['auto', 'sqrt',\n                                                             'log2'],\n                                        'rf__n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C520>},\n                   random_state=42, verbose=2)",
      "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[(&#x27;rf&#x27;, RandomForestClassifier())]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],\n                                        &#x27;rf__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n                                        &#x27;rf__max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C1F0&gt;,\n                                        &#x27;rf__max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n                                                             &#x27;log2&#x27;],\n                                        &#x27;rf__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C520&gt;},\n                   random_state=42, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[(&#x27;rf&#x27;, RandomForestClassifier())]),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;rf__bootstrap&#x27;: [True, False],\n                                        &#x27;rf__class_weight&#x27;: [&#x27;balanced&#x27;, None],\n                                        &#x27;rf__max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C1F0&gt;,\n                                        &#x27;rf__max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n                                                             &#x27;log2&#x27;],\n                                        &#x27;rf__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002E60025C520&gt;},\n                   random_state=42, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rf&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "rf_random = RandomizedSearchCV(estimator=pipeline, param_distributions=param_dist, n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "rf_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5278008298755187\n"
     ]
    }
   ],
   "source": [
    "best_model = rf_random.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators: 104\n",
      "Best max_features: sqrt\n",
      "Best max_depth: 4\n",
      "Best bootstrap: False\n",
      "Best class_weight: None\n"
     ]
    }
   ],
   "source": [
    "best_rf = best_model.named_steps['rf']\n",
    "\n",
    "best_n_estimators = best_rf.n_estimators\n",
    "best_max_features = best_rf.max_features\n",
    "best_max_depth = best_rf.max_depth\n",
    "best_bootstrap = best_rf.bootstrap\n",
    "best_class_weight = best_rf.class_weight\n",
    "\n",
    "print(\"Best n_estimators:\", best_n_estimators)\n",
    "print(\"Best max_features:\", best_max_features)\n",
    "print(\"Best max_depth:\", best_max_depth)\n",
    "print(\"Best bootstrap:\", best_bootstrap)\n",
    "print(\"Best class_weight:\", best_class_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}