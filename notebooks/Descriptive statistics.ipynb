{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.draw import dispersion_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "            id ticker                                              title  \\\n0       221515    NIO  Why Shares of Chinese Electric Car Maker NIO A...   \n1       221516    NIO  NIO only consumer gainer  Workhorse Group amon...   \n2       221517    NIO  NIO leads consumer gainers  Beyond Meat and Ma...   \n3       221518    NIO                  NIO  NVAX among premarket gainers   \n4       221519    NIO                  PLUG  NIO among premarket gainers   \n...        ...    ...                                                ...   \n221508  443024      T                     Crude And Steel Still In Sync    \n221509  443025      T  Forget AT T  This Is The Telecom Stock You Sho...   \n221510  443026      T  Wall Street Exposed   Part 3   How Dividends C...   \n221511  443027      T       Weighing The Week Ahead  It s All About Jobs   \n221512  443028      T  Leap Wireless  LEAP    JPM Note and AT T   T M...   \n\n       category                                            content  \\\n0          news  What s happening\\nShares of Chinese electric c...   \n1          news  Gainers  NIO  NYSE NIO   7  \\nLosers  MGP Ingr...   \n2          news  Gainers  NIO  NYSE NIO   14   Village Farms In...   \n3          news  Cemtrex  NASDAQ CETX   85  after FY results \\n...   \n4          news  aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...   \n...         ...                                                ...   \n221508  opinion  We have been reporting on the trade off betwee...   \n221509  opinion  It s the largest cell phone provider in the wo...   \n221510  opinion  Before we dicuss how the mechanism of dividend...   \n221511  opinion  From start to finish  the coming week will hav...   \n221512  opinion  Leap Wireless International  Inc   Leap  is a ...   \n\n       release_date         provider  \\\n0        2020-01-15  The Motley Fool   \n1        2020-01-18    Seeking Alpha   \n2        2020-01-15    Seeking Alpha   \n3        2020-01-15    Seeking Alpha   \n4        2020-01-06    Seeking Alpha   \n...             ...              ...   \n221508   2012-10-04       Ivan Kitov   \n221509   2012-05-30  StreetAuthority   \n221510   2012-07-16   Portfolio Cafe   \n221511   2012-09-02      Jeff Miller   \n221512   2011-12-31   Ophir Gottlieb   \n\n                                                      url  article_id  \n0                                  https://invst.ly/pigqi     2060327  \n1                                  https://invst.ly/pje9c     2062196  \n2                                  https://invst.ly/pifmv     2060249  \n3                                  https://invst.ly/picu8     2060039  \n4       https://seekingalpha.com/news/3529772-plug-nio...     2053096  \n...                                                   ...         ...  \n221508  https://www.investing.com/analysis/crude-and-s...      138733  \n221509  https://www.investing.com/analysis/forget-at-t...      124829  \n221510  https://www.investing.com/analysis/wall-street...      129651  \n221511  https://www.investing.com/analysis/weighing-th...      134926  \n221512  https://www.investing.com/analysis/leap-wirele...      110079  \n\n[221505 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ticker</th>\n      <th>title</th>\n      <th>category</th>\n      <th>content</th>\n      <th>release_date</th>\n      <th>provider</th>\n      <th>url</th>\n      <th>article_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>221515</td>\n      <td>NIO</td>\n      <td>Why Shares of Chinese Electric Car Maker NIO A...</td>\n      <td>news</td>\n      <td>What s happening\\nShares of Chinese electric c...</td>\n      <td>2020-01-15</td>\n      <td>The Motley Fool</td>\n      <td>https://invst.ly/pigqi</td>\n      <td>2060327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>221516</td>\n      <td>NIO</td>\n      <td>NIO only consumer gainer  Workhorse Group amon...</td>\n      <td>news</td>\n      <td>Gainers  NIO  NYSE NIO   7  \\nLosers  MGP Ingr...</td>\n      <td>2020-01-18</td>\n      <td>Seeking Alpha</td>\n      <td>https://invst.ly/pje9c</td>\n      <td>2062196</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>221517</td>\n      <td>NIO</td>\n      <td>NIO leads consumer gainers  Beyond Meat and Ma...</td>\n      <td>news</td>\n      <td>Gainers  NIO  NYSE NIO   14   Village Farms In...</td>\n      <td>2020-01-15</td>\n      <td>Seeking Alpha</td>\n      <td>https://invst.ly/pifmv</td>\n      <td>2060249</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>221518</td>\n      <td>NIO</td>\n      <td>NIO  NVAX among premarket gainers</td>\n      <td>news</td>\n      <td>Cemtrex  NASDAQ CETX   85  after FY results \\n...</td>\n      <td>2020-01-15</td>\n      <td>Seeking Alpha</td>\n      <td>https://invst.ly/picu8</td>\n      <td>2060039</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>221519</td>\n      <td>NIO</td>\n      <td>PLUG  NIO among premarket gainers</td>\n      <td>news</td>\n      <td>aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...</td>\n      <td>2020-01-06</td>\n      <td>Seeking Alpha</td>\n      <td>https://seekingalpha.com/news/3529772-plug-nio...</td>\n      <td>2053096</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>221508</th>\n      <td>443024</td>\n      <td>T</td>\n      <td>Crude And Steel Still In Sync</td>\n      <td>opinion</td>\n      <td>We have been reporting on the trade off betwee...</td>\n      <td>2012-10-04</td>\n      <td>Ivan Kitov</td>\n      <td>https://www.investing.com/analysis/crude-and-s...</td>\n      <td>138733</td>\n    </tr>\n    <tr>\n      <th>221509</th>\n      <td>443025</td>\n      <td>T</td>\n      <td>Forget AT T  This Is The Telecom Stock You Sho...</td>\n      <td>opinion</td>\n      <td>It s the largest cell phone provider in the wo...</td>\n      <td>2012-05-30</td>\n      <td>StreetAuthority</td>\n      <td>https://www.investing.com/analysis/forget-at-t...</td>\n      <td>124829</td>\n    </tr>\n    <tr>\n      <th>221510</th>\n      <td>443026</td>\n      <td>T</td>\n      <td>Wall Street Exposed   Part 3   How Dividends C...</td>\n      <td>opinion</td>\n      <td>Before we dicuss how the mechanism of dividend...</td>\n      <td>2012-07-16</td>\n      <td>Portfolio Cafe</td>\n      <td>https://www.investing.com/analysis/wall-street...</td>\n      <td>129651</td>\n    </tr>\n    <tr>\n      <th>221511</th>\n      <td>443027</td>\n      <td>T</td>\n      <td>Weighing The Week Ahead  It s All About Jobs</td>\n      <td>opinion</td>\n      <td>From start to finish  the coming week will hav...</td>\n      <td>2012-09-02</td>\n      <td>Jeff Miller</td>\n      <td>https://www.investing.com/analysis/weighing-th...</td>\n      <td>134926</td>\n    </tr>\n    <tr>\n      <th>221512</th>\n      <td>443028</td>\n      <td>T</td>\n      <td>Leap Wireless  LEAP    JPM Note and AT T   T M...</td>\n      <td>opinion</td>\n      <td>Leap Wireless International  Inc   Leap  is a ...</td>\n      <td>2011-12-31</td>\n      <td>Ophir Gottlieb</td>\n      <td>https://www.investing.com/analysis/leap-wirele...</td>\n      <td>110079</td>\n    </tr>\n  </tbody>\n</table>\n<p>221505 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/us_equities_news_dataset.csv')\n",
    "articles = articles[articles['content'].notna()]\n",
    "articles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Descriptive statistics\n",
    "- How many documents\n",
    "- How many words\n",
    "- How much lexical variation in the texts\n",
    "- Average sentence length\n",
    "- Average document length\n",
    "- 100 most common words\n",
    "- 20 most indicative words per class\n",
    "- Dispersion plot of 'Macbook', iPhone and Chromecast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1\n",
    "- Documents\n",
    "- Words\n",
    "- Lexical variation\n",
    "- Sentence Length\n",
    "- Document length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#number of documents\n",
    "nr_of_docu = articles['id'].nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Count words in each row\n",
    "articles['word_count'] = articles['content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#Sum the number of words of each row\n",
    "total_word_count = articles['word_count'].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Create a set of the words, these are the unique words (these are split since it gives a memory error)\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "# Tokenize and count unique words in smaller chunks\n",
    "chunk_size = 1000  # Adjust this value based on your available memory and DataFrame size\n",
    "\n",
    "for i in range(0, len(articles), chunk_size):\n",
    "    chunk = articles.loc[i:i+chunk_size-1]  # Get a chunk of the DataFrame\n",
    "    combined_text = ' '.join(chunk['content'])  # Combine text from the chunk\n",
    "    chunk_words = combined_text.split()  # Tokenize the chunk\n",
    "    unique_words.update(chunk_words)  # Add unique words to the set\n",
    "\n",
    "# Count the total number of unique words\n",
    "unique_word_count = len(unique_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lexical_variation = unique_word_count/total_word_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Average document length\n",
    "avg_doc_length = total_word_count/nr_of_docu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[\\n]+|[ ]{2,}(?=[A-Z][a-z])', text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Apply the count_sentences function to the 'content' column and save the counts in a new column\n",
    "articles['sentence_count'] = articles['content'].apply(count_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "nr_of_sentences = articles['sentence_count'].sum()\n",
    "avg_sentence_length = total_word_count/nr_of_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "avg_doc_length_sentence = nr_of_sentences/nr_of_docu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2: preprocessed data\n",
    "- Common words\n",
    "- Dispersion plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Common words filtering must be done on the preprocessed dataset\n",
    "articles_prep = pd.read_csv('../data/preprocessed_article_data.csv')\n",
    "articles_prep = articles_prep[articles_prep['content'].notna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Combine all text from the 'content' column into a single string\n",
    "combined_text_prep = ' '.join(articles_prep['content'])\n",
    "\n",
    "# Tokenize the combined text into words\n",
    "words_prep = combined_text_prep.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Use Counter to count word frequencies\n",
    "word_counts_prep = Counter(words_prep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "list_common_words = word_counts_prep.most_common(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# # Tokenize the content in each row and store it in a list\n",
    "# tokenized_texts = [nltk.word_tokenize(text) for text in articles_prep['content']]\n",
    "#\n",
    "# # Create an NLTK Text object\n",
    "# text = nltk.Text(word for tokens in tokenized_texts for word in tokens)\n",
    "#\n",
    "# # Create a dispersion plot for specific words\n",
    "# target_words = [\"Macbook\", \"iPhone\", \"Chromecast\"]\n",
    "# dispersion_plot(text, target_words, ignore_case=True)\n",
    "#\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('dispersion_plot.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data_variables = {\n",
    "    \"Number of documents:\" : nr_of_docu,\n",
    "    \"Total number of words:\": total_word_count,\n",
    "    \"Number unique words:\": unique_word_count,\n",
    "    \"Lexical variation: \": round(lexical_variation, 5),\n",
    "    \"Average sentence length is \": [round(avg_sentence_length,2), \"Words\"],\n",
    "    \"Average document length \": [round(avg_doc_length,2), \"Words\"],\n",
    "    \"Average document length \": [round(avg_doc_length_sentence,2), \"Sentences\"],\n",
    "}\n",
    "\n",
    "dataf_variables = pd.DataFrame(data_variables)\n",
    "dataf_variables.to_csv('Analysis statistics.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data written to common_words.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = 'common_words.csv'  # The name of the output CSV file\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_file, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row (if needed)\n",
    "    # csv_writer.writerow(['Header1', 'Header2', ...])\n",
    "\n",
    "    # Write the data from the list of tuples\n",
    "    for item in list_common_words:\n",
    "        csv_writer.writerow(item)\n",
    "\n",
    "print(f'CSV data written to {output_file}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3: prediction data\n",
    "- 20 most indicative words for each class (20 most common words for stock increase 1 or 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Load the prediction dataset\n",
    "articles_pred = pd.read_csv('../data/prediction_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Separate dataframes\n",
    "df_stock_increase_1 = articles_pred.loc[articles_pred['stock_increase'] == 1, ['content']]\n",
    "df_stock_increase_0 = articles_pred.loc[articles_pred['stock_increase'] == 0, ['content']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#First for increase = 1\n",
    "\n",
    "# Combine all text from the 'content' column into a single string\n",
    "combined_text_pred_1 = ' '.join(df_stock_increase_1['content'])\n",
    "\n",
    "# Tokenize the combined text into words\n",
    "words_pred_1 = combined_text_pred_1.split()\n",
    "# Use Counter to count word frequencies\n",
    "word_counts_pred_1 = Counter(words_pred_1)\n",
    "list_common_words_pred_1 = word_counts_pred_1.most_common(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#First for increase = 0\n",
    "\n",
    "# Combine all text from the 'content' column into a single string\n",
    "combined_text_pred_0 = ' '.join(df_stock_increase_0['content'])\n",
    "\n",
    "# Tokenize the combined text into words\n",
    "words_pred_0 = combined_text_pred_0.split()\n",
    "# Use Counter to count word frequencies\n",
    "word_counts_pred_0 = Counter(words_pred_0)\n",
    "list_common_words_pred_0 = word_counts_pred_0.most_common(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert all variables to a CSV file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_variables_2 = {\n",
    "    \"20 most indicative words 1\" : list_common_words_pred_1,\n",
    "    \"20 most indicative words 0\" : list_common_words_pred_0,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataf_variables_2 = pd.DataFrame(data_variables_2)\n",
    "dataf_variables_2.to_csv('Analysis statistics_2.csv', index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}